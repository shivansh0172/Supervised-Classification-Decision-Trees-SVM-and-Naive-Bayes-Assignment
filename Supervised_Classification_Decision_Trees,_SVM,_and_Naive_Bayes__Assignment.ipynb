{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "OYnuaLncMl6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "Information Gain (IG) is a metric used in Decision Trees to decide which feature should be used to split the data at each node.\n",
        "It measures how much uncertainty (entropy) is reduced after splitting the dataset based on a feature.\n",
        "\n",
        "Formula:\n",
        "Information Gain\n",
        "=\n",
        "Entropy(parent)\n",
        "âˆ’\n",
        "âˆ‘\n",
        "(\n",
        "samples in child\n",
        "samples in parent\n",
        "Ã—\n",
        "Entropy(child)\n",
        ")\n",
        "Information Gain=Entropy(parent)âˆ’âˆ‘(\n",
        "samples in parent\n",
        "samples in child\n",
        "\tâ€‹\n",
        "\n",
        "Ã—Entropy(child))\n",
        "\n",
        "How it is used:\n",
        "\n",
        "Decision Trees calculate IG for all features.\n",
        "\n",
        "The feature with the highest Information Gain is selected for splitting.\n",
        "\n",
        "This process repeats recursively until stopping criteria are met.\n",
        "\n",
        "Key Point:\n",
        "\n",
        "Higher Information Gain â‡’ Better split â‡’ More pure child nodes."
      ],
      "metadata": {
        "id": "BiZ7UpMJMqEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?"
      ],
      "metadata": {
        "id": "tVt35ZabMqVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1ï¸âƒ£ Gini Impurity\n",
        "ğŸ‘‰ What it measures\n",
        "\n",
        "Probability of incorrect classification of a randomly chosen data point if it is labeled randomly according to class distribution.\n",
        "\n",
        "ğŸ‘‰ Formula\n",
        "Gini\n",
        "=\n",
        "1\n",
        "âˆ’\n",
        "âˆ‘\n",
        "ğ‘\n",
        "ğ‘–\n",
        "2\n",
        "Gini=1âˆ’âˆ‘p\n",
        "i\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "\n",
        "where\n",
        "ğ‘\n",
        "ğ‘–\n",
        "p\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        " = probability of class i\n",
        "\n",
        "ğŸ‘‰ Key points\n",
        "\n",
        "Range: 0 to 0.5 (binary classification)\n",
        "\n",
        "0 = Pure node (only one class)\n",
        "\n",
        "Faster to compute\n",
        "\n",
        "Used in CART (Classification and Regression Trees)\n",
        "\n",
        "2ï¸âƒ£ Entropy\n",
        "ğŸ‘‰ What it measures\n",
        "\n",
        "Amount of disorder or uncertainty in the dataset.\n",
        "\n",
        "ğŸ‘‰ Formula\n",
        "Entropy\n",
        "=\n",
        "âˆ’\n",
        "âˆ‘\n",
        "ğ‘\n",
        "ğ‘–\n",
        "log\n",
        "â¡\n",
        "2\n",
        "(\n",
        "ğ‘\n",
        "ğ‘–\n",
        ")\n",
        "Entropy=âˆ’âˆ‘p\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "log\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "(p\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        ")\n",
        "ğŸ‘‰ Key points\n",
        "\n",
        "Range: 0 to 1\n",
        "\n",
        "0 = Pure node\n",
        "\n",
        "Uses logarithms â†’ slightly slower\n",
        "\n",
        "Used in ID3 and C4.5 algorithms\n",
        "\n",
        "Based on Information Theory"
      ],
      "metadata": {
        "id": "fp16w9jhMqj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:What is Pre-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "_UsRuPQbMqw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "Pre-Pruning is a technique used to stop a Decision Tree from growing too deep during training to prevent overfitting.\n",
        "\n",
        "Common pre-pruning methods:\n",
        "\n",
        "Limiting maximum depth (max_depth)\n",
        "\n",
        "Minimum samples per node (min_samples_split)\n",
        "\n",
        "Minimum samples per leaf (min_samples_leaf)\n",
        "\n",
        "Maximum number of leaf nodes\n",
        "\n",
        "Benefits:\n",
        "\n",
        "Reduces overfitting\n",
        "\n",
        "Improves generalization\n",
        "\n",
        "Faster training"
      ],
      "metadata": {
        "id": "IxhWOJEHMq9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n"
      ],
      "metadata": {
        "id": "np3xSpGVNPp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train Decision Tree with Gini impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(data.feature_names, model.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WnTAQzXNWck",
        "outputId": "ac8fdeef-bf6d-4401-c2b0-70d710bbed18"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0133\n",
            "sepal width (cm): 0.0000\n",
            "petal length (cm): 0.5641\n",
            "petal width (cm): 0.4226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n"
      ],
      "metadata": {
        "id": "a4P8VFHoNP9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression.\n",
        "\n",
        "Key idea:\n",
        "\n",
        "Finds the optimal hyperplane that separates classes.\n",
        "\n",
        "Maximizes the margin between data points of different classes.\n",
        "\n",
        "Uses support vectors, which are data points closest to the decision boundary.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Effective in high-dimensional spaces\n",
        "\n",
        "Works well with clear margins\n",
        "\n",
        "Robust to overfitting"
      ],
      "metadata": {
        "id": "SnvmQS52NQJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?"
      ],
      "metadata": {
        "id": "TXAkdiCeNQXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "The Kernel Trick allows SVMs to solve non-linearly separable problems by transforming data into a higher-dimensional space without explicitly computing the transformation.\n",
        "\n",
        "Common kernels:\n",
        "\n",
        "Linear\n",
        "\n",
        "Polynomial\n",
        "\n",
        "RBF (Gaussian)\n",
        "\n",
        "Sigmoid\n",
        "\n",
        "Why itâ€™s useful:\n",
        "\n",
        "Enables complex decision boundaries\n",
        "\n",
        "Computationally efficient\n",
        "\n",
        "Avoids explicit feature transformation"
      ],
      "metadata": {
        "id": "TXQ6qv0UNklU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies."
      ],
      "metadata": {
        "id": "3TKfAnvGNmku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Linear SVM\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "linear_pred = svm_linear.predict(X_test)\n",
        "\n",
        "# RBF SVM\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "rbf_pred = svm_rbf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Linear SVM Accuracy:\", accuracy_score(y_test, linear_pred))\n",
        "print(\"RBF SVM Accuracy:\", accuracy_score(y_test, rbf_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN5nPNaeNn_t",
        "outputId": "342ec4f6-e6f1-4158-bd2a-4e8772311012"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM Accuracy: 0.9814814814814815\n",
            "RBF SVM Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the NaÃ¯ve Bayes classifier, and why is it called \"NaÃ¯ve\"?\n"
      ],
      "metadata": {
        "id": "a1_E_UywNzDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "NaÃ¯ve Bayes is a probabilistic classifier based on Bayesâ€™ Theorem.\n",
        "\n",
        "Why â€œNaÃ¯veâ€?\n",
        "\n",
        "Because it assumes that all features are independent, which is rarely true in real-world data.\n",
        "\n",
        "Formula:\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ¶\n",
        "ğ‘™\n",
        "ğ‘\n",
        "ğ‘ \n",
        "ğ‘ \n",
        "âˆ£\n",
        "ğ¹\n",
        "ğ‘’\n",
        "ğ‘\n",
        "ğ‘¡\n",
        "ğ‘¢\n",
        "ğ‘Ÿ\n",
        "ğ‘’\n",
        "ğ‘ \n",
        ")\n",
        "âˆ\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ¶\n",
        "ğ‘™\n",
        "ğ‘\n",
        "ğ‘ \n",
        "ğ‘ \n",
        ")\n",
        "Ã—\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ¹\n",
        "ğ‘’\n",
        "ğ‘\n",
        "ğ‘¡\n",
        "ğ‘¢\n",
        "ğ‘Ÿ\n",
        "ğ‘’\n",
        "ğ‘ \n",
        "âˆ£\n",
        "ğ¶\n",
        "ğ‘™\n",
        "ğ‘\n",
        "ğ‘ \n",
        "ğ‘ \n",
        ")\n",
        "P(Classâˆ£Features)âˆP(Class)Ã—P(Featuresâˆ£Class)\n",
        "Advantages:\n",
        "\n",
        "Simple and fast\n",
        "\n",
        "Works well with small datasets\n",
        "\n",
        "Effective for text classification"
      ],
      "metadata": {
        "id": "-5a2-Lw2N1Oj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian NaÃ¯ve Bayes, Multinomial NaÃ¯ve\n",
        "Bayes, and Bernoulli NaÃ¯ve Bayes"
      ],
      "metadata": {
        "id": "llGBWgdyN5Ei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Gaussian NaÃ¯ve Bayes\n",
        "\n",
        "Assumes that features follow a normal (Gaussian) distribution.\n",
        "\n",
        "Used for continuous numerical data.\n",
        "\n",
        "Common in medical, biological, and sensor-based datasets.\n",
        "\n",
        "Example: Height, weight, temperature, breast cancer features.\n",
        "\n",
        "2. Multinomial NaÃ¯ve Bayes\n",
        "\n",
        "Designed for discrete count data.\n",
        "\n",
        "Works well when features represent frequency or counts.\n",
        "\n",
        "Mostly used in text classification problems.\n",
        "\n",
        "Example: Word counts in documents, term frequency in emails.\n",
        "\n",
        "3. Bernoulli NaÃ¯ve Bayes\n",
        "\n",
        "Works with binary features (0 or 1).\n",
        "\n",
        "Considers whether a feature is present or absent.\n",
        "\n",
        "Useful when feature occurrence matters more than frequency.\n",
        "\n",
        "Example: Spam detection (word present or not)."
      ],
      "metadata": {
        "id": "XhkHLhYrN-Gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Python program â€“ Gaussian NaÃ¯ve Bayes (Breast Cancer Dataset)"
      ],
      "metadata": {
        "id": "o5vIZVO6N-hN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gaussian Naive Bayes\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cjbtYqPOAyT",
        "outputId": "ae953e25-7334-4ad6-bc1a-8f8908d507ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9415204678362573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9PT9_jqWOC73"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}